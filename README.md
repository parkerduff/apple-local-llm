# apple-local-llm

Call Apple's on-device Foundation Models using the OpenAI Responses API format â€” no servers, no setup.

## Status

ðŸš§ **Under development** â€” this package is reserved and not yet functional.

## Coming Soon

- Drop-in OpenAI Responses API compatibility
- Works with Node.js, Electron, and VS Code extensions
- No localhost server required
- Automatic fallback when unavailable
